{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Actor Critic methods in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, alpha, beta, gamma=0.99, n_actions=4, layer1_size=1024, layer2_size=512, input_dims=8):\n",
    "        self.gamma=gamma\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "        self.n_actions=n_actions\n",
    "        self.input_dims=input_dims\n",
    "        self.fc1_dims=layer1_size\n",
    "        self.fc2_dims=layer2_size\n",
    "        \n",
    "        self.actor, self.critic, self.policy = self.build_actor_critic_network()\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "        \n",
    "    def build_actor_critic_network(self):\n",
    "        input=Input(shape=(self.input_dims,))\n",
    "        delta=Input(shape=[1])\n",
    "        dense1=Dense(self.fc1_dims,activation='relu')(input)\n",
    "        dense2=Dense(self.fc2_dims,activation='relu')(dense1)\n",
    "        probs = Dense(self.n_actions,activation='softmax')(dense2)\n",
    "        values = Dense(1,activation='linear')(dense2)\n",
    "        \n",
    "        def custom_loss(y_true,y_pred):    \n",
    "            out = K.clip(y_pred,1e-8,1-1e-8)\n",
    "            log_lik=y_true*K.log(out)\n",
    "            return K.sum(-log_lik*delta)\n",
    "        \n",
    "        actor =Model(inputs=[input,delta],outputs=[probs])\n",
    "        \n",
    "        actor.compile(optimizer=Adam(learning_rate=self.alpha), loss=custom_loss)\n",
    "        \n",
    "        critic =Model(inputs=[input],outputs=[values])\n",
    "        \n",
    "        critic.compile(optimizer=Adam(learning_rate=self.beta), loss='mean_squared_error')\n",
    "        \n",
    "        policy = Model(inputs=[input],outputs=[probs])\n",
    "        \n",
    "        return actor, critic, policy\n",
    "\n",
    "    def choose_action(self,observation):\n",
    "        state = observation[np.newaxis,:]\n",
    "        probabilities = self.policy.predict(state)[0]\n",
    "        action =np.random.choice(self.action_space,p=probabilities)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, state,action, reward, state_, done):\n",
    "        state=state[np.newaxis,:]\n",
    "        state_=state_[np.newaxis,:]\n",
    "        \n",
    "        critic_value=self.critic.predict(state)\n",
    "        critic_value_=self.critic.predict(state_)\n",
    "        target=reward + self.gamma*critic_value_*(1-int(done))\n",
    "        \n",
    "        delta = target - critic_value\n",
    "        \n",
    "        actions =np.zeros([1,self.n_actions])\n",
    "        actions[np.arange(1),action]=1.0\n",
    "\n",
    "        \n",
    "        self.actor.fit([state,delta],actions,verbose=0)\n",
    "        self.critic.fit(state,target,verbose=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=Agent(alpha=0.00001, beta=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('LunarLander-v2')#, render_mode=\"human\")\n",
    "score_history=[]\n",
    "num_episodes=10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(num_episodes):\n",
    "    done=False\n",
    "    score=0\n",
    "    observation=env.reset()[0]\n",
    "    \n",
    "    while not done:\n",
    "        action=agent.choose_action(observation)\n",
    "        observation_,reward,done,info=env.step(action)[0:4]\n",
    "        agent.learn(observation, action, reward, observation_,done)\n",
    "        observation=observation_\n",
    "        score = score+reward\n",
    "    \n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    print('episode ',i,'score %.2f average score %.2f' % (score,avg_score) )\n",
    "\n",
    "filename='lunar-lander-actor-critic.png'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256.34622022996086\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('LunarLander-v2', render_mode=\"human\")\n",
    "done=False\n",
    "score=0\n",
    "observation=env.reset()[0]\n",
    "    \n",
    "while not done:\n",
    "     action=agent.choose_action(observation)\n",
    "     observation_,reward,done,info=env.step(action)[0:4]\n",
    "     observation=observation_\n",
    "     score += reward\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
